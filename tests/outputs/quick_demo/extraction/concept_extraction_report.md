# ğŸ§  æ¦‚å¿µæå–ä¸çŸ¥è¯†å›¾è°±æŠ¥å‘Š

## ğŸ“Š æå–ç»Ÿè®¡
- å¤„ç†è®ºæ–‡æ•°: 1
- æ ¸å¿ƒæ¦‚å¿µæ€»æ•°: 7
- å…±åŒæ¦‚å¿µæ•°: 6

## ğŸ¯ æ¦‚å¿µå±‚æ¬¡ç»“æ„
### 1
- æ·±åº¦å­¦ä¹ (Deep Learning)
- ç¥ç»ç½‘ç»œ(Neural Networks)
- è‡ªç„¶è¯­è¨€å¤„ç†(NLP)
- æœºå™¨å­¦ä¹ (Machine Learning)

### 2
- Transformeræ¨¡å‹
- å¤šå¤´è‡ªæ³¨æ„åŠ›(multi-head self-attention)
- åºåˆ—åˆ°åºåˆ—æ¨¡å‹(sequence-to-sequence models)

### 3
- è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanism)
- ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(scaled dot-product attention)
- è‡ªå›å½’æ¨¡å‹(auto-regressive models)

## ğŸ“š æ¨èå­¦ä¹ é¡ºåº

## ğŸ­ æ¦‚å¿µèšç±»
### ç»´åº¦-ä½œç”¨çš„çŸ¥è¯†é›†ç¾¤
- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- ç‚¹ç§¯æŠ€æœ¯

## ğŸ”— è®ºæ–‡ä¾èµ–å…³ç³»
- **paper_1** â†’ **paper_2**
  - åŸå› : ä»è‡ªç„¶è¯­è¨€å¤„ç†çš„ç†è®ºå¦‚Transformerçš„åŸºç¡€é€æ­¥è¿›å…¥é¢†åŸŸåº”ç”¨

## ğŸ“‹ å„è®ºæ–‡æ¦‚å¿µè¯¦æƒ…
### paper_1
- **éš¾åº¦**: advanced
- **å­¦ä¹ æ—¶é—´**: 240 åˆ†é’Ÿ
- **é¢†åŸŸ**: è‡ªç„¶è¯­è¨€å¤„ç†, æ·±åº¦å­¦ä¹ , æœºå™¨å­¦ä¹ 
- **æ ¸å¿ƒæ¦‚å¿µ**: Transformeræ¨¡å‹, å¤šå¤´è‡ªæ³¨æ„åŠ›(multi-head self-attention), è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanism), ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚(encoder-decoder attention layers), åºåˆ—åˆ°åºåˆ—æ¨¡å‹(sequence-to-sequence models)
- **å‰ç½®çŸ¥è¯†**: 4 é¡¹
