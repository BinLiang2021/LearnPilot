# ğŸ“ AI-Paper-Tutor å­¦ä¹ æŠ¥å‘Š

## ğŸ‘¤ å­¦ä¹ é…ç½®
- å­¦ä¹ æ°´å¹³: intermediate
- æ¯æ—¥æ—¶é—´: 2 å°æ—¶
- å­¦ä¹ å¤©æ•°: 5 å¤©
- å­¦ä¹ ç›®æ ‡: å¿«é€Ÿç†è§£Transformeræ¶æ„

## ğŸ“š è®ºæ–‡æ¦‚è§ˆ
- è®ºæ–‡æ€»æ•°: 1
- éš¾åº¦åˆ†å¸ƒ:
  - advanced: 1 ç¯‡
- æ€»é¢„ä¼°å­¦ä¹ æ—¶é—´: 80 åˆ†é’Ÿ (1.3 å°æ—¶)

## ğŸ§  æ ¸å¿ƒæ¦‚å¿µ
### ğŸ¯ é«˜é¢‘æ ¸å¿ƒæ¦‚å¿µ
- **Transformeræ¨¡å‹** (å‡ºç° 1 æ¬¡)
- **å¤šå¤´è‡ªæ³¨æ„åŠ›(multi-head self-attention)** (å‡ºç° 1 æ¬¡)
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanism)** (å‡ºç° 1 æ¬¡)
- **ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚(encoder-decoder attention layers)** (å‡ºç° 1 æ¬¡)
- **åºåˆ—åˆ°åºåˆ—æ¨¡å‹(sequence-to-sequence models)** (å‡ºç° 1 æ¬¡)
- **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(scaled dot-product attention)** (å‡ºç° 1 æ¬¡)
- **æŸå¤±å‡½æ•°æ­£åˆ™åŒ–æŠ€æœ¯(e.g., label-smoothing)** (å‡ºç° 1 æ¬¡)

### ğŸŒ æ¶‰åŠé¢†åŸŸ
- è‡ªç„¶è¯­è¨€å¤„ç† (1 ç¯‡è®ºæ–‡)
- æ·±åº¦å­¦ä¹  (1 ç¯‡è®ºæ–‡)
- æœºå™¨å­¦ä¹  (1 ç¯‡è®ºæ–‡)

## ğŸ’¡ å­¦ä¹ å»ºè®®
- åœ¨å¼€å§‹é˜…è¯»æœ¬æ–‡çŒ®ä¹‹å‰ï¼Œç¡®ä¿æ‚¨å¯¹ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†æœ‰æ‰å®çš„ç†è§£ã€‚
- æ·±å…¥ç†è§£åºåˆ—å»ºæ¨¡å’Œè½¬å½•ä»»åŠ¡çš„ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚
- å¯ä»¥ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°Transformeræ¶æ„ï¼Œä»¥å¢å¼ºå¯¹å…¶å·¥ä½œåŸç†çš„å®é™…ç†è§£ã€‚
- ç³»ç»Ÿæ€§å¤ä¹ æ³¨æ„æœºåˆ¶çš„åŸºæœ¬æ¦‚å¿µï¼Œä»¥ä¾¿å®Œå…¨ç†è§£æ–‡ä¸­æå‡ºçš„æ”¹è¿›ç‚¹ã€‚

## ğŸ“‹ æ¨èå­¦ä¹ è·¯å¾„
### ç¬¬1æ­¥: Attention Is All You Need
- **éš¾åº¦**: advanced
- **å­¦ä¹ æ—¶é—´**: 80 åˆ†é’Ÿ
- **æ ¸å¿ƒæ¦‚å¿µ**: Attention mechanism, Transformer architecture, Sequence transduction tasks
- **å­¦ä¹ é‡ç‚¹**: Development of the Transformer architecture based on multi-head self-attention mechanisms and position-wise feed-forward networks, evaluated primarily on machine translation tasks.

## ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜
- `analysis/` - è®ºæ–‡åˆ†æè¯¦ç»†ç»“æœ
- `extraction/` - æ¦‚å¿µæå–å’ŒçŸ¥è¯†å›¾è°±
- `pipeline_report.md` - æœ¬å­¦ä¹ æŠ¥å‘Š

---
*ç”± AI-Paper-Tutor è‡ªåŠ¨ç”Ÿæˆ*