"""
KnowledgeExtractor å®é™…è¿è¡Œæµ‹è¯•
æµ‹è¯•æ¦‚å¿µæå–å™¨çš„åŠŸèƒ½ï¼Œä½¿ç”¨ Attention Is All You Need è®ºæ–‡ä½œä¸ºæ¡ˆä¾‹
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.learn_pilot.agents.knowledge_extractor import KnowledgeExtractor, extract_concepts_from_papers
from src.learn_pilot.literature_utils.markdown_parser import parse_papers_from_directory
import logging

logging.basicConfig(level=logging.INFO)

async def test_knowledge_extractor():
    """æµ‹è¯•æ¦‚å¿µæå–å™¨"""
    print("ğŸ§  æµ‹è¯• KnowledgeExtractor")
    print("=" * 50)
    
    # è®¾ç½®è·¯å¾„
    input_dir = "tests/test_marldown_folder"
    output_dir = "tests/outputs/knowledge_extraction"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # é¦–å…ˆè§£æè®ºæ–‡
        print("ğŸ“– è§£æè®ºæ–‡æ–‡ä»¶...")
        papers = parse_papers_from_directory(input_dir)
        
        if not papers:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ–‡ä»¶")
            return None
        
        print(f"âœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        for i, paper in enumerate(papers, 1):
            print(f"   {i}. {paper.title}")
        
        # æ–¹æ³•1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
        print("\nğŸ” æ–¹æ³•1: ä½¿ç”¨ä¾¿æ·å‡½æ•° extract_concepts_from_papers")
        results = await extract_concepts_from_papers(papers, output_dir)
        
        print("\nâœ… æ¦‚å¿µæå–å®Œæˆï¼ç»“æœæ¦‚è§ˆ:")
        print(f"ğŸ“Š å¤„ç†è®ºæ–‡æ•°é‡: {len(results.get('extractions', {}))}")
        
        # å±•ç¤ºç¬¬ä¸€ç¯‡è®ºæ–‡çš„æ¦‚å¿µæå–ç»“æœ
        extractions = results.get('extractions', {})
        if extractions:
            first_paper_id = list(extractions.keys())[0]
            first_extraction = extractions[first_paper_id]['output']
            
            print(f"\nğŸ“– è®ºæ–‡æ¦‚å¿µæå–è¯¦æƒ…:")
            print(f"   - éš¾åº¦è¯„ä¼°: {first_extraction['difficulty_assessment']}")
            print(f"   - æ¦‚å¿µå¤æ‚åº¦: {first_extraction['conceptual_complexity']}")
            print(f"   - å­¦ä¹ æ—¶é—´ä¼°ç®—: {first_extraction['estimated_learning_time']} åˆ†é’Ÿ")
            print(f"   - çŸ¥è¯†é¢†åŸŸ: {', '.join(first_extraction['knowledge_domains'])}")
            
            print(f"\nğŸ¯ æ ¸å¿ƒæ¦‚å¿µ ({len(first_extraction['core_concepts'])} ä¸ª):")
            for i, concept in enumerate(first_extraction['core_concepts'][:8], 1):
                print(f"   {i:2d}. {concept}")
            
            print(f"\nğŸ”— æ”¯æ’‘æ¦‚å¿µ ({len(first_extraction['supporting_concepts'])} ä¸ª):")
            for i, concept in enumerate(first_extraction['supporting_concepts'][:5], 1):
                print(f"   {i:2d}. {concept}")
            
            print(f"\nğŸ“š å‰ç½®çŸ¥è¯†éœ€æ±‚ ({len(first_extraction['prerequisites'])} é¡¹):")
            for i, prereq in enumerate(first_extraction['prerequisites'][:5], 1):
                print(f"   {i:2d}. {prereq['name']} ({prereq['level']}) - {prereq['reason'][:50]}...")
            
            print(f"\nğŸ”€ æ¦‚å¿µå…³ç³» ({len(first_extraction['concept_relationships'])} ä¸ª):")
            for i, rel in enumerate(first_extraction['concept_relationships'][:3], 1):
                print(f"   {i}. {rel['concept1']} --{rel['relationship']}--> {rel['concept2']}")
        
        # å±•ç¤ºè·¨è®ºæ–‡åˆ†æç»“æœ
        cross_analysis = results.get('cross_paper_analysis', {}).get('output', {})
        if cross_analysis:
            print(f"\nğŸŒ è·¨è®ºæ–‡æ¦‚å¿µåˆ†æ:")
            
            if cross_analysis.get('common_concepts'):
                print(f"   - å…±åŒæ¦‚å¿µ ({len(cross_analysis['common_concepts'])} ä¸ª):")
                for concept in cross_analysis['common_concepts'][:5]:
                    print(f"     â€¢ {concept}")
            
            if cross_analysis.get('concept_hierarchy'):
                print(f"   - æ¦‚å¿µå±‚æ¬¡ç»“æ„:")
                hierarchy = cross_analysis['concept_hierarchy']
                for level, concepts in hierarchy.items():
                    print(f"     â€¢ {level}: {len(concepts)} ä¸ªæ¦‚å¿µ")
                    for concept in concepts[:3]:
                        print(f"       - {concept}")
            
            if cross_analysis.get('recommended_sequence'):
                print(f"   - æ¨èå­¦ä¹ é¡ºåº: {cross_analysis['recommended_sequence']}")
            
            if cross_analysis.get('concept_clusters'):
                print(f"   - æ¦‚å¿µèšç±» ({len(cross_analysis['concept_clusters'])} ä¸ªé›†ç¾¤):")
                for cluster_name, concepts in list(cross_analysis['concept_clusters'].items())[:3]:
                    print(f"     â€¢ {cluster_name}: {len(concepts)} ä¸ªæ¦‚å¿µ")
                    for concept in concepts[:2]:
                        print(f"       - {concept}")
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        # è®¡ç®—èµ„æºä½¿ç”¨
        total_cost = 0
        total_tokens = 0
        for paper_id, extraction in extractions.items():
            usage = extraction.get('usage', {})
            total_cost += usage.get('estimated_cost_usd', 0)
            total_tokens += usage.get('total_tokens', 0)
        
        # åŠ ä¸Šè·¨è®ºæ–‡åˆ†æçš„æˆæœ¬
        if results.get('cross_paper_analysis', {}).get('usage'):
            cross_usage = results['cross_paper_analysis']['usage']
            total_cost += cross_usage.get('estimated_cost_usd', 0)
            total_tokens += cross_usage.get('total_tokens', 0)
        
        print(f"\nğŸ’° èµ„æºä½¿ç”¨ç»Ÿè®¡:")
        print(f"   - æ€»tokenæ•°: {total_tokens:,}")
        print(f"   - é¢„ä¼°æˆæœ¬: ${total_cost:.4f}")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_single_paper_extraction():
    """æµ‹è¯•å•ç¯‡è®ºæ–‡çš„è¯¦ç»†æ¦‚å¿µæå–"""
    print("\n" + "="*50)
    print("ğŸ”¬ è¯¦ç»†å•ç¯‡è®ºæ–‡æ¦‚å¿µæå–æµ‹è¯•")
    print("="*50)
    
    try:
        # è§£æè®ºæ–‡
        papers = parse_papers_from_directory("tests/test_marldown_folder")
        if not papers:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ–‡ä»¶")
            return
        
        paper = papers[0]  # å–ç¬¬ä¸€ç¯‡è®ºæ–‡
        print(f"ğŸ“– åˆ†æè®ºæ–‡: {paper.title}")
        
        # åˆ›å»ºæå–å™¨å¹¶åˆ†æ
        extractor = KnowledgeExtractor()
        result = await extractor._extract_concepts_from_paper(paper)
        
        extraction = result['output']
        usage = result['usage']
        
        print(f"\nğŸ“Š è¯¦ç»†æ¦‚å¿µæå–ç»“æœ:")
        print(f"   - éš¾åº¦è¯„ä¼°: {extraction['difficulty_assessment']}")
        print(f"   - æ¦‚å¿µå¤æ‚åº¦: {extraction['conceptual_complexity']}")
        print(f"   - å­¦ä¹ æ—¶é—´: {extraction['estimated_learning_time']} åˆ†é’Ÿ")
        
        print(f"\nğŸŒ çŸ¥è¯†é¢†åŸŸ ({len(extraction['knowledge_domains'])} ä¸ª):")
        for i, domain in enumerate(extraction['knowledge_domains'], 1):
            print(f"   {i:2d}. {domain}")
        
        print(f"\nğŸ¯ æ ¸å¿ƒæ¦‚å¿µ ({len(extraction['core_concepts'])} ä¸ª):")
        for i, concept in enumerate(extraction['core_concepts'], 1):
            print(f"   {i:2d}. {concept}")
        
        print(f"\nğŸ”— æ”¯æ’‘æ¦‚å¿µ ({len(extraction['supporting_concepts'])} ä¸ª):")
        for i, concept in enumerate(extraction['supporting_concepts'], 1):
            print(f"   {i:2d}. {concept}")
        
        print(f"\nğŸ“š å‰ç½®çŸ¥è¯† ({len(extraction['prerequisites'])} ä¸ª):")
        for i, prereq in enumerate(extraction['prerequisites'], 1):
            print(f"   {i:2d}. [{prereq['level']}] {prereq['name']}")
            print(f"        ç†ç”±: {prereq['reason']}")
        
        print(f"\nğŸ”€ æ¦‚å¿µå…³ç³» ({len(extraction['concept_relationships'])} ä¸ª):")
        for i, rel in enumerate(extraction['concept_relationships'], 1):
            print(f"   {i:2d}. {rel['concept1']} --[{rel['relationship']}]--> {rel['concept2']}")
        
        print(f"\nğŸ’° æœ¬æ¬¡æå–èµ„æºä½¿ç”¨:")
        print(f"   - è¾“å…¥tokens: {usage['input_tokens']:,}")
        print(f"   - è¾“å‡ºtokens: {usage['output_tokens']:,}")
        print(f"   - æ€»tokens: {usage['total_tokens']:,}")
        print(f"   - é¢„ä¼°æˆæœ¬: ${usage['estimated_cost_usd']:.4f}")
        
    except Exception as e:
        print(f"âŒ è¯¦ç»†æå–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_cross_paper_analysis():
    """æµ‹è¯•è·¨è®ºæ–‡åˆ†æåŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ”€ è·¨è®ºæ–‡æ¦‚å¿µå…³ç³»åˆ†ææµ‹è¯•")
    print("="*50)
    
    try:
        # è§£æè®ºæ–‡
        papers = parse_papers_from_directory("tests/test_marldown_folder")
        if not papers:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ–‡ä»¶")
            return
        
        # åˆ›å»ºæå–å™¨
        extractor = KnowledgeExtractor()
        
        # å…ˆæå–å„è®ºæ–‡çš„æ¦‚å¿µ
        print("ğŸ” ä¸ºæ¯ç¯‡è®ºæ–‡æå–æ¦‚å¿µ...")
        extractions = {}
        for i, paper in enumerate(papers):
            paper_id = f"paper_{i+1}"
            print(f"   å¤„ç† {paper_id}: {paper.title}")
            result = await extractor._extract_concepts_from_paper(paper)
            extractions[paper_id] = result
        
        print(f"âœ… å®Œæˆ {len(extractions)} ç¯‡è®ºæ–‡çš„æ¦‚å¿µæå–")
        
        # è¿›è¡Œè·¨è®ºæ–‡åˆ†æ
        print("\nğŸŒ æ‰§è¡Œè·¨è®ºæ–‡æ¦‚å¿µå…³ç³»åˆ†æ...")
        cross_result = await extractor._analyze_cross_paper_concepts(extractions)
        
        analysis = cross_result['output']
        usage = cross_result['usage']
        
        print(f"\nğŸ“Š è·¨è®ºæ–‡åˆ†æç»“æœ:")
        
        print(f"\nğŸ¯ å…±åŒæ¦‚å¿µ ({len(analysis['common_concepts'])} ä¸ª):")
        for i, concept in enumerate(analysis['common_concepts'], 1):
            print(f"   {i:2d}. {concept}")
        
        print(f"\nğŸ“Š æ¦‚å¿µå±‚æ¬¡ç»“æ„:")
        hierarchy = analysis['concept_hierarchy']
        for level, concepts in hierarchy.items():
            print(f"   ğŸ“ˆ {level.upper()} ({len(concepts)} ä¸ª):")
            for concept in concepts[:5]:
                print(f"      â€¢ {concept}")
        
        print(f"\nğŸ“š æ¨èå­¦ä¹ é¡ºåº:")
        for i, paper_id in enumerate(analysis['recommended_sequence'], 1):
            print(f"   {i}. {paper_id}")
        
        print(f"\nğŸ”— è®ºæ–‡ä¾èµ–å…³ç³» ({len(analysis['learning_dependencies'])} ä¸ª):")
        for i, dep in enumerate(analysis['learning_dependencies'], 1):
            print(f"   {i}. {dep['prerequisite_paper']} â†’ {dep['dependent_paper']}")
            print(f"      åŸå› : {dep['reason']}")
        
        print(f"\nğŸ­ æ¦‚å¿µèšç±» ({len(analysis['concept_clusters'])} ä¸ªé›†ç¾¤):")
        for cluster_name, concepts in analysis['concept_clusters'].items():
            print(f"   ğŸ“ {cluster_name} ({len(concepts)} ä¸ªæ¦‚å¿µ):")
            for concept in concepts[:4]:
                print(f"      â€¢ {concept}")
        
        print(f"\nğŸ•¸ï¸ çŸ¥è¯†å›¾è°±è¾¹ ({len(analysis['knowledge_graph_edges'])} æ¡):")
        for i, edge in enumerate(analysis['knowledge_graph_edges'][:8], 1):
            print(f"   {i:2d}. {edge['from']} --[{edge['relation']}]--> {edge['to']}")
        
        print(f"\nğŸ’° è·¨è®ºæ–‡åˆ†æèµ„æºä½¿ç”¨:")
        print(f"   - è¾“å…¥tokens: {usage['input_tokens']:,}")
        print(f"   - è¾“å‡ºtokens: {usage['output_tokens']:,}")
        print(f"   - æ€»tokens: {usage['total_tokens']:,}")
        print(f"   - é¢„ä¼°æˆæœ¬: ${usage['estimated_cost_usd']:.4f}")
        
    except Exception as e:
        print(f"âŒ è·¨è®ºæ–‡åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ KnowledgeExtractor åŠŸèƒ½æµ‹è¯•")
    
    # è¿è¡ŒåŸºæœ¬æµ‹è¯•
    results = asyncio.run(test_knowledge_extractor())
    
    # è¿è¡Œè¯¦ç»†æµ‹è¯•
    asyncio.run(test_single_paper_extraction())
    
    # è¿è¡Œè·¨è®ºæ–‡åˆ†ææµ‹è¯•
    asyncio.run(test_cross_paper_analysis())
    
    print("\nğŸ‰ KnowledgeExtractor æµ‹è¯•å®Œæˆï¼")
