"""
Knowledge Extractor Agent
æ¦‚å¿µæå–Agentï¼Œä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ¦‚å¿µæå–å’ŒçŸ¥è¯†åˆ†æ
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
import json

from src.learn_pilot.core.agents.structured_output_agent import StructuredOutputAgent
from src.learn_pilot.core.config.config import OPENAI_API_KEY, LANGUAGE
from src.learn_pilot.models.paper_models import Paper

logger = logging.getLogger(__name__)

class Prerequisite(BaseModel):
    level: str
    name: str
    
class ConceptRelationship(BaseModel):
    concept1: str
    relationship: str
    concept2: str

class ConceptExtractionOutput(BaseModel):
    """æ¦‚å¿µæå–è¾“å‡ºç»“æ„"""
    core_concepts: List[str] = Field(description="æ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨")
    supporting_concepts: List[str] = Field(description="æ”¯æ’‘æ¦‚å¿µåˆ—è¡¨")
    prerequisites: List[Prerequisite] = Field(description="å‰ç½®çŸ¥è¯†è¦æ±‚")
    difficulty_assessment: str = Field(description="éš¾åº¦è¯„ä¼°: beginner, intermediate, advanced")
    conceptual_complexity: str = Field(description="æ¦‚å¿µå¤æ‚åº¦: low, medium, high")
    estimated_learning_time: int = Field(description="å­¦ä¹ æ—¶é—´ä¼°è®¡(åˆ†é’Ÿ)")
    concept_relationships: List[ConceptRelationship] = Field(description="æ¦‚å¿µå…³ç³»")
    knowledge_domains: List[str] = Field(description="çŸ¥è¯†é¢†åŸŸåˆ—è¡¨")

class KnowledgeExtractor:
    """çŸ¥è¯†æå–å™¨Agent - LLMé©±åŠ¨ç‰ˆæœ¬"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logger
        self.model = self.config.get("model", "gpt-4o-2024-11-20")
        
    async def extract_concepts_from_papers(self, papers: List[Paper]) -> Dict[str, Any]:
        """ä»è®ºæ–‡åˆ—è¡¨ä¸­æå–æ¦‚å¿µ"""
        try:
            self.logger.info(f"ğŸ§  å¼€å§‹ä» {len(papers)} ç¯‡è®ºæ–‡ä¸­æå–æ¦‚å¿µ")
            
            # ä¸ºæ¯ç¯‡è®ºæ–‡æå–æ¦‚å¿µ
            extractions = {}
            for i, paper in enumerate(papers):
                paper_id = f"paper_{i+1}"
                self.logger.info(f"ğŸ” æå–è®ºæ–‡æ¦‚å¿µ {paper_id}: {paper.title}")
                
                extraction = await self._extract_concepts_from_paper(paper)
                extractions[paper_id] = extraction
            
            # ç”Ÿæˆè·¨è®ºæ–‡çš„æ¦‚å¿µå…³ç³»åˆ†æ
            cross_paper_analysis = await self._analyze_cross_paper_concepts(extractions)
            
            result = {
                "extractions": extractions,
                "cross_paper_analysis": cross_paper_analysis,
                "paper_ids": list(extractions.keys())
            }
            
            self.logger.info(f"âœ… æ¦‚å¿µæå–å®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ¦‚å¿µæå–å¤±è´¥: {e}")
            raise
    
    async def _extract_concepts_from_paper(self, paper: Paper) -> Dict[str, Any]:
        """ä»å•ç¯‡è®ºæ–‡ä¸­æå–æ¦‚å¿µ"""
        
        # æ„å»ºè®ºæ–‡å†…å®¹
        paper_content = f"""
æ ‡é¢˜: {paper.title}

æ‘˜è¦:
{paper.abstract}

ä¸»è¦ç« èŠ‚å†…å®¹:
"""
        # åªåŒ…å«ä¸»è¦ç« èŠ‚ï¼Œé¿å…å†…å®¹è¿‡é•¿
        main_sections = [s for s in paper.sections if any(keyword in s.title.lower() 
                                                         for keyword in ['introduction', 'method', 'approach', 'model', 'algorithm', 'conclusion'])]
        for section in main_sections[:5]:  # æœ€å¤š5ä¸ªä¸»è¦ç« èŠ‚
            paper_content += f"\n## {section.title}\n{section.content[:2000]}...\n"  # é™åˆ¶æ¯ä¸ªç« èŠ‚é•¿åº¦
        
        # æ„å»ºæ¦‚å¿µæå–prompt
        instructions = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£ä»è®ºæ–‡ä¸­æå–å’Œåˆ†ææ ¸å¿ƒæ¦‚å¿µã€‚

è¯·ä»”ç»†åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œè¯†åˆ«å’Œæå–ï¼š

1. **æ ¸å¿ƒæ¦‚å¿µ**: è®ºæ–‡çš„ä¸»è¦æŠ€æœ¯æ¦‚å¿µã€æ–¹æ³•ã€æ¨¡å‹ç­‰
2. **æ”¯æ’‘æ¦‚å¿µ**: è®ºæ–‡ä¸­æåˆ°çš„ç›¸å…³æŠ€æœ¯å’Œç†è®º
3. **å‰ç½®çŸ¥è¯†**: ç†è§£è¿™ç¯‡è®ºæ–‡éœ€è¦çš„åŸºç¡€çŸ¥è¯†
4. **æ¦‚å¿µå…³ç³»**: æ¦‚å¿µä¹‹é—´çš„ä¾èµ–ã€æ‰©å±•ã€ç›¸ä¼¼å…³ç³»
5. **çŸ¥è¯†é¢†åŸŸ**: è®ºæ–‡æ‰€å±çš„æŠ€æœ¯é¢†åŸŸ

è¯„ä¼°æ ‡å‡†ï¼š
- éš¾åº¦è¯„ä¼°åŸºäºç ”ç©¶ç”Ÿæ°´å¹³
- å­¦ä¹ æ—¶é—´ä¼°ç®—åŒ…æ‹¬ç†è§£å’ŒæŒæ¡
- å‰ç½®çŸ¥è¯†åº”è¯¥å…·ä½“ä¸”å®ç”¨
- æ¦‚å¿µå…³ç³»è¦æ˜ç¡®è¯´æ˜å…³ç³»ç±»å‹

è¯·ç”¨{LANGUAGE}å›ç­”æ‰€æœ‰å†…å®¹ã€‚
"""

        # åˆ›å»ºLLMæå–å™¨
        extractor = StructuredOutputAgent(
            model=self.model,
            api_key=OPENAI_API_KEY,
            instructions=instructions,
            output_type=ConceptExtractionOutput
        )
        
        input_messages = [
            {
                "role": "user", 
                "content": f"è¯·æå–ä»¥ä¸‹è®ºæ–‡çš„æ¦‚å¿µå’ŒçŸ¥è¯†ç»“æ„ï¼š\n\n{paper_content}"
            }
        ]
        
        # æ‰§è¡Œæå–
        result = await extractor.run(input_messages)
        return result
    
    async def _analyze_cross_paper_concepts(self, extractions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè·¨è®ºæ–‡çš„æ¦‚å¿µå…³ç³»"""
        
        # æ”¶é›†æ‰€æœ‰æ¦‚å¿µä¿¡æ¯
        all_concepts_info = []
        for paper_id, extraction in extractions.items():
            output = extraction['output']
            all_concepts_info.append({
                'paper_id': paper_id,
                'core_concepts': output['core_concepts'],
                'supporting_concepts': output['supporting_concepts'],
                'knowledge_domains': output['knowledge_domains'],
                'difficulty': output['difficulty_assessment']
            })
        
        # æ„å»ºè·¨è®ºæ–‡åˆ†æprompt
        cross_analysis_prompt = f"""
åŸºäºä»¥ä¸‹å¤šç¯‡è®ºæ–‡çš„æ¦‚å¿µæå–ç»“æœï¼Œè¯·è¿›è¡Œè·¨è®ºæ–‡çš„æ¦‚å¿µå…³ç³»åˆ†æï¼š

{json.dumps(all_concepts_info, ensure_ascii=False, indent=2)}

è¯·åˆ†æï¼š
1. å…±åŒå‡ºç°çš„æ ¸å¿ƒæ¦‚å¿µ
2. æ¦‚å¿µçš„ä¾èµ–å…³ç³»å›¾
3. è®ºæ–‡é—´çš„çŸ¥è¯†é€’è¿›å…³ç³»
4. æ¨èçš„å­¦ä¹ è·¯å¾„
5. æ¦‚å¿µé›†ç¾¤åˆ†æ

ç”¨{LANGUAGE}æä¾›åˆ†æç»“æœã€‚
"""

        class ConceptCluster(BaseModel):
            concept: str
            related_concepts: List[str] 
            
        class LearningDependency(BaseModel):
            prerequisite_paper: str
            dependent_paper: str
            reason: str
            
        class ConceptHierarchy(BaseModel):
            level: str
            concepts: List[str]
            
        class KnowledgeGraphEdge(BaseModel):
            source: str
            target: str
            relationship: str

        # å®šä¹‰è·¨è®ºæ–‡åˆ†æè¾“å‡ºç»“æ„
        class CrossPaperAnalysisOutput(BaseModel):
            common_concepts: List[str] = Field(description="å…±åŒæ¦‚å¿µåˆ—è¡¨")
            concept_hierarchy: List[ConceptHierarchy] = Field(description="æ¦‚å¿µå±‚æ¬¡ç»“æ„")
            learning_dependencies: List[LearningDependency] = Field(description="å­¦ä¹ ä¾èµ–å…³ç³»")
            recommended_sequence: List[str] = Field(description="æ¨èå­¦ä¹ é¡ºåº")
            concept_clusters: List[ConceptCluster] = Field(description="æ¦‚å¿µèšç±»")
            knowledge_graph_edges: List[KnowledgeGraphEdge] = Field(description="çŸ¥è¯†å›¾è°±è¾¹")

        analyzer = StructuredOutputAgent(
            model=self.model,
            api_key=OPENAI_API_KEY,
            instructions="ä½ æ˜¯çŸ¥è¯†å›¾è°±ä¸“å®¶ï¼Œè¯·åˆ†æè®ºæ–‡é—´çš„æ¦‚å¿µå…³ç³»å¹¶æ„å»ºå­¦ä¹ è·¯å¾„ã€‚",
            output_type=CrossPaperAnalysisOutput
        )
        
        result = await analyzer.run([{"role": "user", "content": cross_analysis_prompt}])
        return result
    
    def save_extraction_results(self, results: Dict[str, Any], output_dir: str):
        """ä¿å­˜æ¦‚å¿µæå–ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†æå–ç»“æœ
        extraction_file = os.path.join(output_dir, "concept_extraction.json")
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        serializable_results = {
            "extractions": {},
            "cross_paper_analysis": results.get("cross_paper_analysis", {}),
            "paper_ids": results.get("paper_ids", [])
        }
        
        # è½¬æ¢æå–ç»“æœ
        for paper_id, result in results.get("extractions", {}).items():
            serializable_results["extractions"][paper_id] = {
                "output": result["output"],
                "usage": result["usage"]
            }
        
        with open(extraction_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ¦‚å¿µå›¾è°±æŠ¥å‘Š
        report = self._generate_concept_report(results)
        report_file = os.path.join(output_dir, "concept_extraction_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ§  æ¦‚å¿µæå–ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _generate_concept_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¦‚å¿µæå–æŠ¥å‘Š"""
        lines = ["# ğŸ§  æ¦‚å¿µæå–ä¸çŸ¥è¯†å›¾è°±æŠ¥å‘Š", ""]
        
        extractions = results.get("extractions", {})
        cross_analysis = results.get("cross_paper_analysis", {}).get("output", {})
        
        # åŸºæœ¬ç»Ÿè®¡
        lines.append("## ğŸ“Š æå–ç»Ÿè®¡")
        lines.append(f"- å¤„ç†è®ºæ–‡æ•°: {len(extractions)}")
        
        total_core_concepts = sum(len(e["output"]["core_concepts"]) for e in extractions.values())
        lines.append(f"- æ ¸å¿ƒæ¦‚å¿µæ€»æ•°: {total_core_concepts}")
        
        if cross_analysis.get("common_concepts"):
            lines.append(f"- å…±åŒæ¦‚å¿µæ•°: {len(cross_analysis['common_concepts'])}")
        
        lines.append("")
        
        # æ¦‚å¿µå±‚æ¬¡ç»“æ„
        if cross_analysis.get("concept_hierarchy"):
            lines.append("## ğŸ¯ æ¦‚å¿µå±‚æ¬¡ç»“æ„")
            hierarchy = cross_analysis["concept_hierarchy"]
            for data in hierarchy:
                level = data['level']
                concepts = data['concepts']
                lines.append(f"### {level.title()}")
                for concept in concepts[:8]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    lines.append(f"- {concept}")
                lines.append("")
        
        # æ¨èå­¦ä¹ é¡ºåº
        if cross_analysis.get("recommended_sequence"):
            lines.append("## ğŸ“š æ¨èå­¦ä¹ é¡ºåº")
            for i, paper_id in enumerate(cross_analysis["recommended_sequence"], 1):
                if paper_id in extractions:
                    output = extractions[paper_id]["output"]
                    difficulty = output["difficulty_assessment"]
                    time = output["estimated_learning_time"]
                    domains = ", ".join(output["knowledge_domains"][:3])
                    lines.append(f"{i}. **{paper_id}** ({difficulty}, {time}åˆ†é’Ÿ)")
                    lines.append(f"   - é¢†åŸŸ: {domains}")
                    lines.append(f"   - æ ¸å¿ƒæ¦‚å¿µ: {', '.join(output['core_concepts'][:3])}...")
            lines.append("")
        
        # æ¦‚å¿µèšç±»
        if cross_analysis.get("concept_clusters"):
            lines.append("## ğŸ­ æ¦‚å¿µèšç±»")
            clusters = cross_analysis["concept_clusters"]
            for data in clusters:
                concept = data['concept']
                related_concepts = data['related_concepts']
                lines.append(f"### {concept}")
                for concept in related_concepts[:6]:
                    lines.append(f"- {concept}")
                lines.append("")
        
        # å­¦ä¹ ä¾èµ–å…³ç³»
        if cross_analysis.get("learning_dependencies"):
            lines.append("## ğŸ”— è®ºæ–‡ä¾èµ–å…³ç³»")
            for dep in cross_analysis["learning_dependencies"][:5]:
                lines.append(f"- **{dep['prerequisite_paper']}** â†’ **{dep['dependent_paper']}**")
                lines.append(f"  - åŸå› : {dep['reason']}")
            lines.append("")
        
        # å„è®ºæ–‡è¯¦æƒ…
        lines.append("## ğŸ“‹ å„è®ºæ–‡æ¦‚å¿µè¯¦æƒ…")
        for paper_id, extraction in extractions.items():
            output = extraction["output"]
            lines.append(f"### {paper_id}")
            lines.append(f"- **éš¾åº¦**: {output['difficulty_assessment']}")
            lines.append(f"- **å­¦ä¹ æ—¶é—´**: {output['estimated_learning_time']} åˆ†é’Ÿ")
            lines.append(f"- **é¢†åŸŸ**: {', '.join(output['knowledge_domains'])}")
            lines.append(f"- **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(output['core_concepts'][:5])}")
            lines.append(f"- **å‰ç½®çŸ¥è¯†**: {len(output['prerequisites'])} é¡¹")
            lines.append("")
        
        return "\n".join(lines)

# ä¾¿æ·å‡½æ•°
async def extract_concepts_from_papers(papers: List[Paper], output_dir: str = None) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šä»è®ºæ–‡åˆ—è¡¨æå–æ¦‚å¿µ"""
    extractor = KnowledgeExtractor()
    results = await extractor.extract_concepts_from_papers(papers)
    
    if output_dir:
        extractor.save_extraction_results(results, output_dir)
    
    return results

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test():
        extractor = KnowledgeExtractor()
        print("âœ… KnowledgeExtractor åˆå§‹åŒ–å®Œæˆ")
    
    asyncio.run(test()) 