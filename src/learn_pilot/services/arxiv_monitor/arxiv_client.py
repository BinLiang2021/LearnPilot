"""
@file_name: arxiv_client.py
@author: bin.liang
@date: 2025-06-28
@description: ArXiv APIå®¢æˆ·ç«¯ - è´Ÿè´£ä»arXivæ£€ç´¢è®ºæ–‡æ•°æ®
"""

import requests
import feedparser
from typing import List, Dict, Any
from src.learn_pilot.core.logging.logger import logger


class ArxivClient:
    """ArXiv APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = 'http://export.arxiv.org/api/query'):
        self.base_url = base_url
    
    async def fetch_papers(
        self, 
        category: str, 
        search_term: str = '', 
        max_results: int = 2000
    ) -> List[Dict[str, Any]]:
        """
        ä»arXivæ£€ç´¢è®ºæ–‡
        
        Args:
            category: arXivç±»åˆ«ä»£ç  (å¦‚ 'cs.AI')
            search_term: é¢å¤–æœç´¢è¯
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢
            query = f'cat:{category}'
            if search_term:
                query += f'+AND+all:{search_term}'
            
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            logger.info(f"ğŸ” æ£€ç´¢arXivè®ºæ–‡: category={category}, search_term={search_term}")
            
            # å‘é€è¯·æ±‚
            response = requests.get(self.base_url, params=params)
            response.raise_for_status()
            
            # è§£æRSS Feed
            feed = feedparser.parse(response.text)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            papers = []
            for entry in feed.entries:
                paper = {
                    'title': entry.title,
                    'summary': entry.summary,
                    'url': entry.id,
                    'authors': [author.name for author in getattr(entry, 'authors', [])],
                    'arxiv_id': entry.id.split('/')[-1],
                    'published': getattr(entry, 'published', ''),
                    'links': [{'href': link.href, 'type': getattr(link, 'type', '')} for link in getattr(entry, 'links', [])]
                }
                papers.append(paper)
            
            logger.info(f"ğŸ“š æˆåŠŸæ£€ç´¢åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            logger.error(f"âŒ arXivæ£€ç´¢å¤±è´¥: {str(e)}")
            return [] 